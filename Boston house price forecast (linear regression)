#梯度下降算法
import numpy as np
import matplotlib.pyplot as plt
def func(x):
    return x ** 2 + 1
epochs=50
ir=0.1
xi=-18
def get_gradient(x):
    return x*2
trajectory = []
def get_x_star(xi):
    for i in range(epochs):
        trajectory.append(xi)
        xi=xi-ir*get_gradient(xi)
    x_star=xi
    return x_star
get_gradient(xi)
x=np.arange(-20,20,0.1)
y=func(x)
plt.plot(x,y)
x_trajectory=np.array(trajectory)
y_trajectory=func(x_trajectory)
plt.scatter(x_trajectory,y_trajectory)
plt.show()

#波士顿房价预测（线性回归）
from sklearn.datasets import load_boston
import matplotlib.pyplot as plt
import random
import numpy as np
data = load_boston()
X, Y = data['data'], data['target']
room_index = 5
X_rm = X[:, room_index]
print(X.shape)
# 求偏导
# 对k求
def partial_k(y_ture, y_guess, x):
    return -2 * np.mean((np.array(y_ture) - np.array(y_guess)) * np.array(x))
# 对b求
def partial_b(y_ture, y_guess):
    return -2 * np.mean((np.array(y_ture) - np.array(y_guess)))
# 求损失
def l2_loss(y_ture, y_guess):
    return np.mean((np.array(y_ture) - np.array(y_guess)) ** 2)
# y_hat=k*x+b
def y_guess(k, x, b):
    return k * x + b
trying_time = 20000
min_loss = float('inf')
best_k, best_b = None, None
learning_rate = 1e-4
k = random.randint(0, 50)
b = random.randint(-50, 50)
plt.figure()
plt.scatter(X_rm, Y, color='red', alpha=0.5)
plt.figure()
plt.scatter(X_rm, Y, color='red', alpha=0.5)
plt.plot(X_rm, y_guess(k, X_rm, b), color='green')
plt.show()
for i in range(trying_time):
    # 将当前损失于最小损失相比较
    yhat = y_guess(k, X_rm, b)
    L2_loss = l2_loss(Y, yhat)
    if L2_loss < min_loss:
        best_k = k
        best_b = b
        min_loss = L2_loss
    # 找更合适的k与b
    k = k - partial_k(Y, yhat, X_rm) * learning_rate
    b = b - partial_b(Y, yhat) * learning_rate
print('L2loss=', min_loss)
plt.scatter(X_rm, Y, color='red')
plt.plot(X_rm, y_guess(best_k, X_rm, best_b), color='blue')
print('y = {} * x {}'.format(best_k, best_b))
plt.show()
#k与b为随机生成的两个数，模型的准确性要多试几次
'''L2loss= 46.66045888795766
y = 6.624675762640142 * x -18.9113757122038'''
